---
title: "Analysis of Google Play Store Apps"
author: "Minhaz Khan"
date: "January 12, 2019"
output:
  html_document: default
  pdf_document: default
---

```{r}
library(tidyverse)
library(lubridate)
library(klaR)
library(broom)
library(tidytext)
library(rpart)
library(rpart.plot)
library(modelr)
```
##Introduction

Have you ever come across an app, saw it had a 4.5 star rating and it described exactly what you were looking for but then found out the app was extremely buggy? After taking a closer look at the app page you find out it has about a thousand installs and only a couple of hundred reviews. Majority of apps in the Google Play store and any app store in general are misleading like this due to several factors such as number of installs and reviews as mentioned above along with when it was last updated, the OS version it supports and etc. In this analysis we will be exploring the true rating of the app. 

#Reading in the Data
```{r}
apps = read_csv("googleplaystore.csv")
head(apps)
```

#Average rating of each genre

Filtered out the missing rating values and stored them in a new data frame which we will use for the rest of the analysis.

```{r}
apps %>% filter(Rating != 'NaN') -> true_apps
true_apps %>% group_by(Genres) %>% summarise(avg=mean(Rating))
```
Most genres have a decent ratng around 4.0 but majority of these genres are similar to each other, something that we will simplify later on.

Getting a glimpse of the data to convert the numerical data from being characters
```{r}
glimpse(true_apps)
```
We have a lot of character variables here that we need as numbers, we will be converting them below

##Seperating Numerical and Character values from columns
```{r}
true_apps %>% filter(Size != "Varies with device") %>% 
  separate(Size, c("Size","Type"), sep = -1, convert = TRUE) %>% 
  separate(Installs, c("Installs","Symbol"), sep = -1, convert = TRUE) %>% drop_na() -> apps2

apps2$Price = parse_number(apps2$Price)

apps2$Symbol = NULL
apps2$Category = NULL
apps2$`Current Ver`= NULL
```
Removed Symbol because it was just character; removed category because it was the same as genre, removed Current Version because it isn't as necessary as when the app was last updated. Kept Android version because exploring app compatibility with OS might be interesting.

#Converting character values to numeric
The convert parameter in seperate didn't work so here we are manually converting Installs and Size to numeric variables
```{r}
apps2$Installs = as.numeric(gsub(",","",apps2$Installs))
apps2$Size = as.numeric(as.character(apps2$Size))
glimpse(apps2)
```

#Converting dates
```{r}

apps2$`Last Updated` = gsub(",","",apps2$`Last Updated`)
apps2 %>% mutate(`Last Update` = mdy(`Last Updated`)) -> apps2

```

Attempting to factor prices as paid or not:
```{r}
apps2 %>% mutate(Price = case_when(
                              Price == 0 ~ Price, 
                              Price > 0 ~ Price/Price,
                              TRUE ~ NA_real_)) -> apps2

```

#Converting kilobyte app size
```{r}
apps2 %>% mutate(SIZE = case_when(
                              Type == "M" ~ Size, 
                              Type == "k" ~ Size/1024,
                              TRUE ~ NA_real_)) -> apps2

apps2 %>% separate_rows(Genres, sep = ";", convert = FALSE) -> apps2

apps2$Size = NULL
apps2$Type = NULL
```
The case_when function helped out here because values in the columns to be changed aren't the same.


#Some Descriptive Statistics
```{r}
apps2 %>% group_by(Genres) %>% summarise(m=mean(Rating))
(apps_aov = aov(Rating~SIZE+Installs+Price+Reviews+Genres+`Content Rating`+`Last Update`, data = apps2))

summary(apps_aov)

```
From the results of the analysis of Variance we observe there is a significant difference in the means in all the categories except for Content Rating, given their p-values are far less than 0.05.

##Graphing the relationship between the rating and other numerical factors
```{r}
ggplot(apps2, aes(Rating,log(Installs)))+geom_point()+xlim(0,5)+ylim(min(log(apps2$Installs)),max(log(apps2$Installs)))+geom_smooth()
ggplot(apps2, aes(Rating,log(Reviews)))+geom_point()+xlim(0,5)+ylim(min(log(apps2$Reviews)),max(log(apps2$Reviews)))+geom_smooth()
ggplot(apps2, aes(Rating,SIZE))+geom_point()+xlim(0,5)+ylim(min(apps2$SIZE),max(apps2$SIZE))+geom_smooth()
ggplot(apps2, aes(Rating,Price))+geom_point()+xlim(0,5)+ylim(min(apps2$Price),max(apps2$Price))+geom_smooth()
#appplot + ylim(0,5) + xlim(1,1.0e+09)
#appplot + ylim(0,5)
```
Effective rating goes down after reaching its peak in proportion to the categories above as expected. This highlights the fact that good apps exist but doesn't get much attention or the ratings and reviews are fabricated.

## Regression Models

#Fixing duplicate Genres under similar terms
```{r}
apps2 %>% mutate(Genres = case_when(
                              Genres == "Educational" ~ "Education", 
                              Genres == "Music & Audio" ~ "Music",
                              Genres == "Music & Video" ~ "Music",
                              TRUE ~ Genres)) -> apps2
```


```{r}
appreg = lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+Genres+Price, data = apps2, weights = Installs)
summary(appreg)
```
Taking the log of Installs and Reviews and weighing it with the Installs helped increase the R squared to about 53% from under 1%
```{r}
ggplot(appreg, aes(x=.fitted, y=.resid))+geom_point()+geom_smooth()
```
This is a peculiar residual plot due to the fact there is a specific range for the ratings (0-5), there seems to be a certain pattern in its overall shape but it's difficult to judge the overall randomness.

##Kmeans for Genres

Here we attempt K-means clustering to create our mega genres to reduce the overwhelming number of genres we have. We are gonna cluster them into 10 specific genres as we feel it's a good amount of variety.
```{r}
#Here we use the kmodes function from the kLaR package that is suitable for categorical variable clusterting like our genres
# genres = kmodes(apps2[,c(3:7,11)], modes = 10, iter.max = 10, weighted = FALSE)
# 
# #Each 
# plot(jitter(apps2$Rating),col = genres$cluster)
# points(genres$modes, col = 1:5, pch = 8)
# genres$modes
# genres$cluster

#I commented this part out because it kept crashing due to memory issues


```
```{r}
genres = readRDS("genres.rds")
```
However I'm loading the cluster that had the preferable generated genres to be used for the rest of the analysis.
```{r}
apps2 %>% mutate(cluster=genres$cluster) -> apps2
```
```{r}
clustapp = genres$modes
```

```{r}
apps2 %>% group_by(cluster, Genres) %>% count() %>% summarise(m=max(n)) %>% summarise(mm=max(m))  
```
```{r}
#Adding the new mega genres to the dataset
apps2 %>% mutate(True_Genre = case_when(
                                         cluster == 1 & Genres != clustapp$Genres[1] ~ clustapp$Genres[1],
                                         cluster == 2 & Genres != clustapp$Genres[2] ~ clustapp$Genres[2],
                                         cluster == 3 & Genres != clustapp$Genres[3] ~ clustapp$Genres[3],
                                         cluster == 4 & Genres != clustapp$Genres[4] ~ clustapp$Genres[4],
                                         cluster == 5 & Genres != clustapp$Genres[5] ~ clustapp$Genres[5],
                                         cluster == 6 & Genres != clustapp$Genres[6] ~ clustapp$Genres[6],
                                         cluster == 7 & Genres != clustapp$Genres[7] ~ clustapp$Genres[7],
                                         cluster == 8 & Genres != clustapp$Genres[8] ~ clustapp$Genres[8],
                                         cluster == 9 & Genres != clustapp$Genres[9] ~ clustapp$Genres[9],
                                         cluster == 10 & Genres != clustapp$Genres[10] ~ clustapp$Genres[10],
                                         TRUE ~ Genres)) -> apps3

```

```{r}
apps3 %>% group_by(True_Genre) %>% count()
```

##Regression Model with the new Genres:

```{r}
newreg = lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+True_Genre+Price, data = apps3, weights = Installs)
summary(newreg)
```
R-squared was lowered and for a good reason since we had an over abundance of genres previously that was inflating the value. 

```{r}
apps3 %>% filter(Rating >= 1 & Rating < 5) -> apps4 
reg1 = lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+True_Genre-1+Price, data = apps4, weights = Installs)
summary(reg1)
```
Here we opted to model app ratings that are in the range of [1,5) since it is a more realistic range. The extremely high R-squared is misleadinsignificant because majority of the apps are free. We also observe there is a negative slope for installs and positive slope for the number of reviews which confirms some of our hypotheses in the introduction where a decent app has a lot of installs but not enough reviews to have the score it truly deserves. As expected, the "Last Update" is also significant because it affects the overall rating of the app if the developers don't update to fix the bugs, so the more updated an app is the better the rating hence the positive slope. 


```{r}
ggplot(reg1, aes(x=.fitted, y=.resid))+geom_point()+geom_smooth()
```
The residual plot doesn't look much different from before except it's less condensed at the top since we are not considering 5 star ratings.

#Making regression models for each genre of apps

```{r}
apps4 %>% filter(True_Genre == "Action") %>% lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+Price, data = ., weights = Installs) %>% tidy() 
apps4 %>% filter(True_Genre == "Business") %>% lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+Price, data = ., weights = Installs) %>% tidy() 
apps4 %>% filter(True_Genre == "Dating") %>% lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+Price, data = ., weights = Installs) %>% tidy() 
apps4 %>% filter(True_Genre == "Education") %>% lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+Price, data = ., weights = Installs) %>% tidy() 
apps4 %>% filter(True_Genre == "Finance") %>% lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+Price, data = ., weights = Installs) %>% tidy() 
apps4 %>% filter(True_Genre == "Health & Fitness") %>% lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+Price, data = ., weights = Installs) %>% tidy() 
apps4 %>% filter(True_Genre == "Lifestyle") %>% lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+Price, data = ., weights = Installs) %>% tidy() 
apps4 %>% filter(True_Genre == "Productivity") %>% lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+Price, data = ., weights = Installs) %>% tidy() 
apps4 %>% filter(True_Genre == "Shopping") %>% lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+Price, data = ., weights = Installs) %>% tidy() 
apps4 %>% filter(True_Genre == "Tools") %>% lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+Price, data = ., weights = Installs) %>% tidy() 
```
Here we can observe some of the factors aren't significant at all for some genres while some p-values are borderline zero like in the Action games genre where the p-values for Installs and Reviews are to the power of 200+ while Price and Size aren't significant. Most apps are free so the p-value for price is understandable and size isn't significant to the ratings because every kind of game has its own required size. Last update for action games is significant because if the game isn't patched from bugs soonthen ratings plummet fast.

Last update isn't significant on fiance apps because they are usually made a for a select set of functions and as long as they are performing right, updates aren't need as frequently other than to sort out major nugs or OS optimization. 

Tools app has a highly significant p-value with reviews and that's because tools apps are concerned with managing the phone as a whole like boosting battery life and file management so if they aren't working as intended consumers will report their issues which significantly affect the ratings. 


##Sentiment Analysis
Here we explore user reviews and visualize their positive and negative reviews and attempt to connect them to our previous modelss
```{r}
#Reading and cleaning the data
reviews = read_csv("googleplaystore_user_reviews.csv")
(reviews = reviews %>% drop_na())
```


```{r}
reviews %>% ggplot(aes(x=Sentiment,y=Sentiment_Subjectivity))+geom_boxplot()
```

#Seperating every review into Individual Words
```{r}
(tidy_reviews <- reviews %>% filter(Sentiment != "Neutral") %>% group_by(Sentiment) %>% unnest_tokens(word,Translated_Review))
```

```{r}
#Counting the most common terms after filtering out common words
data("stop_words")
tidy_reviews = tidy_reviews %>% anti_join(stop_words)
```

```{r}
tidy_reviews %>% group_by(Sentiment) %>% count(word,sort = TRUE) %>% filter(n > 500) %>% 
  mutate(word=reorder(word,n)) %>% 
  ggplot(aes(word,n))+geom_col()+xlab(NULL)+coord_flip()
```
Majority of the frequently occuring words are general terms used for apps and genres like "app" and "games"

#WordCloud
```{r}
library(wordcloud)
```
```{r}
tidy_reviews %>% filter(Sentiment=="Positive") %>% count(word) %>% with(wordcloud(word,n,max.words = 175))
```
```{r}
tidy_reviews %>% filter(Sentiment=="Negative") %>% count(word) %>% with(wordcloud(word,n,max.words = 175))
```
As expected both word clouds contains common terms that describes apps in general but also their own terms that correspond to their sentiment.

```{r}
review_apps = apps4 %>% inner_join(reviews)
review_apps %>% filter(Sentiment != "Neutral") -> review_apps
```
#WordClouds for each genre
```{r}
library(reshape2)
review_apps %>% filter(True_Genre=="Education") %>% unnest_tokens(word,Translated_Review) %>% count(word,Sentiment,sort = TRUE) %>%  
  acast(word ~ Sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

review_apps %>% filter(True_Genre=="Action") %>% unnest_tokens(word,Translated_Review) %>% count(word,Sentiment,sort = TRUE) %>%  
  acast(word ~ Sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

review_apps %>% filter(True_Genre=="Business") %>% unnest_tokens(word,Translated_Review) %>% count(word,Sentiment,sort = TRUE) %>%  
  acast(word ~ Sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

review_apps %>% filter(True_Genre=="Dating") %>% unnest_tokens(word,Translated_Review) %>% count(word,Sentiment,sort = TRUE) %>%  
  acast(word ~ Sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

review_apps %>% filter(True_Genre=="Finance") %>% unnest_tokens(word,Translated_Review) %>% count(word,Sentiment,sort = TRUE) %>%  
  acast(word ~ Sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

review_apps %>% filter(True_Genre=="Health & Fitness") %>% unnest_tokens(word,Translated_Review) %>% count(word,Sentiment,sort = TRUE) %>%  
  acast(word ~ Sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

review_apps %>% filter(True_Genre=="Lifestyle") %>% unnest_tokens(word,Translated_Review) %>% count(word,Sentiment,sort = TRUE) %>%  
  acast(word ~ Sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

review_apps %>% filter(True_Genre=="Productivity") %>% unnest_tokens(word,Translated_Review) %>% count(word,Sentiment,sort = TRUE) %>%  
  acast(word ~ Sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

review_apps %>% filter(True_Genre=="Shopping") %>% unnest_tokens(word,Translated_Review) %>% count(word,Sentiment,sort = TRUE) %>%  
  acast(word ~ Sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

review_apps %>% filter(True_Genre=="Tools") %>% unnest_tokens(word,Translated_Review) %>% count(word,Sentiment,sort = TRUE) %>%  
  acast(word ~ Sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
  
```
Unfortunately some of the word clouds couldn't be properly generated however we again see common terms for apps appear across all the word clouds but this time we have terms that specific to each genre in both positive and negative light. For example for Action games (just games in general actually) we have positivie terms like "good" and "graphics" while negative terms like "connection" which mainly corresponds to games that require an online connection to play and server connection issues are plenty (Looking at you EA). 

My favorite one is for Dating genre where you see a nice big "fake" from the negative side which represents all dating apps because they keep their platform alive by generating fake profiles which most users don't take kindly to after finding out.
```{r}
#Some experimentation with fitting sentiment variables to a model
review_apps %>% group_by(Sentiment) %>% count()
review_apps %>% mutate(Sentiment3=cut(Sentiment_Polarity, breaks=c(-2,0,2))) %>% 
  glm(Sentiment3~Sentiment_Subjectivity,family = "binomial",data = .) %>% tidy()
```

```{r}
reg2 = lm(Rating~SIZE+I(log(Installs))+I(log(Reviews))+`Last Update`+Price+Sentiment, data = review_apps, weights = Installs)
summary(reg2)
```
From this model we can see all the factors are significant from before except for price due to most apps being free. The R-squared is at a decent 84% as well. Here we have our Sentiment variable applied and we can observe from its estimate that app ratings increase by about 0.0125 for every positive review compared to negative reviews. The standard error for it is pretty low so we can trust this estimate. 

```{r}
#Categorized the ratings into specific ranges
review_apps %>% mutate(rating_int=cut(Rating,breaks = c(0,1,2,3,4,5))) -> review_apps
```

##Conclusion
```{r}
review_apps %>% filter(Sentiment != "Neutral") %>% ggplot(aes(rating_int, fill=Sentiment))+geom_bar()+facet_wrap(~True_Genre,scales = "free")

review_apps %>% filter(Sentiment != "Neutral") %>% ggplot(aes(rating_int, fill=Sentiment))+geom_bar(position = "fill")+facet_wrap(~True_Genre,scales = "free")
```

Here is a simple visualization that ties our sentiment analysis with our response variable rating from before. There are several interesting things to note here with the most eye catching one being the presence of negative reviews in (4,5] star ratings. While there are significantly more positive reviews in all categories, we have to remember we are talking about the "Sentiment" of the review here which means someone can give a good rating to the app but their review in discussing the flaws of the app make it a negative review.

Looking at the proportions, the distrubtuions vary between ratings and each genre with some of them being roughly equal while others increase in positive reviews the higher the rating is.

In conclusion, my original aim for this analysis was to model a formula that calculates the effective rating of an app based on factors discussed here but that proved to be too ambitious. Instead I ended up exploring the relationships the app ratings have with variables like the  number of installs and reviews along with the sentiment of reviews to explain how these variables statistically affect the app's overall rating so users aren't misled by what they see at first glance. 
